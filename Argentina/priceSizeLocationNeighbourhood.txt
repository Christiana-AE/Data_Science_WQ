4. Predicting Price with Size, Location, and Neighborhood


Libraries used below 

import warnings
from glob import glob

import pandas as pd
import seaborn as sns
import wqet_grader
from category_encoders import OneHotEncoder
from IPython.display import VimeoVideo
from ipywidgets import Dropdown, FloatSlider, IntSlider, interact
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Ridge  # noqa F401
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.utils.validation import check_is_fitted

warnings.simplefilter(action="ignore", category=FutureWarning)
wqet_grader.init("Project 2 Assessment")

------------------------------------------------------------------------------------

Task
Calculate the number of unique values for each non-numeric feature in df
Drop low and high cardinality fields from the dataframe 

df.select_dtypes("object").nunique()

** We use dtypes as object to check for non numeric items 


# Drop low and high cardinality variables i.e. non numerical fields containing low or too high numbers of unique values 
# Dropping as these don't tell us anything distinguishing to use for our model prediction

df.drop(columns=["operation", "property_type", "currency", "properati_url"], inplace = True)

------------------------------------------------------------------------------------

Task

Modify your wrangle function to drop any features that would constitute leakage.
Leakage is the use of data in training your model that would not be typically be available when making predictions. 
For example, suppose we want to predict property prices in USD but include property prices in Mexican Pesos in our model.


# Drop columns with leakage
# These are columns that give us hints / information about our target feature (feature we are trying to predict - price)
# Don't need these columns as these may not be available in real world data

df.drop(columns = ["price", "price_aprox_local_currency","price_per_m2","price_usd_per_m2"], inplace = True)

------------------------------------------------------------------------------------

Task

Plot a correlation heatmap of the remaining numerical features in df. Since "price_aprox_usd" will be your target, you don't need to include it in your heatmap.

corr = df.select_dtypes("number").drop(columns = "price_aprox_usd").corr()

#use seaborn to visualise
sns.heatmap(corr)


# We see high multicollinearity between 'surface_covered_in_m2', 'surface_total_in_m2' and rooms so we remove these from the dataframe
# Multicolinearity is when there is a strong association / correlation between two or more independent variables in your dataset
df.drop(columns = ['surface_total_in_m2', 'rooms'], inplace = True)


------------------------------------------------------------------------------------

Task - Split Data

Create your feature matrix X_train and target vector y_train. Your target is "price_aprox_usd". 
Your features should be all the columns that remain in the DataFrame you cleaned above.

target = "price_aprox_usd"
features = ["surface_covered_in_m2", "lat", "lon", "neighborhood"]

y_train = df[target]
X_train = df[features]


------------------------------------------------------------------------------------

Task - Baseline

Calculate the baseline mean absolute error for your model.

y_mean = y_train.mean()
y_predict_baseline = [y_mean] * len(y_train)

print("Mean apt price:", round(y_mean, 2))

print("Baseline MAE:", mean_absolute_error(y_train, y_predict_baseline))


------------------------------------------------------------------------------------

Task

Create a pipeline named model that contains a OneHotEncoder, SimpleImputer, and Ridge predictor.

model = make_pipeline(
    OneHotEncoder(use_cat_names = True),
    SimpleImputer(),
    Ridge()
)
model.fit(X_train, y_train)

------------------------------------------------------------------------------------

Task - Evaluate Model

Calculate the training mean absolute error for your predictions as compared to the true targets in y_train.

y_pred_training = model.predict(X_train)
print("Training MAE:", mean_absolute_error(y_train, y_pred_training))

------------------------------------------------------------------------------------

Task - Communicate result

Create a function make_prediction that takes four arguments (area, lat, lon, and neighborhood) and returns your model's prediction for an apartment price.

def make_prediction(area, lat, lon, neighborhood):
    data = {
        "surface_covered_in_m2" : area,
        "lat" : lat,
        "lon" : lon,
        "neighborhood" : neighborhood
    }
    df = pd.DataFrame(data, index=[0])
    prediction = model.predict(df).round(2)[0]
    return f"Predicted apartment price: ${prediction}"


------------------------------------------------------------------------------------

Task - Communicate result using Interactive Dashboard

Add your make_prediction to the interact widget below, run the cell, and then adjust the widget to see how predicted apartment price changes.
Create an interact function in Jupyter Widgets.

interact(
    make_prediction,
    area=IntSlider(
        min=X_train["surface_covered_in_m2"].min(),
        max=X_train["surface_covered_in_m2"].max(),
        value=X_train["surface_covered_in_m2"].mean(),
    ),
    lat=FloatSlider(
        min=X_train["lat"].min(),
        max=X_train["lat"].max(),
        step=0.01,
        value=X_train["lat"].mean(),
    ),
    lon=FloatSlider(
        min=X_train["lon"].min(),
        max=X_train["lon"].max(),
        step=0.01,
        value=X_train["lon"].mean(),
    ),
    neighborhood=Dropdown(options=sorted(X_train["neighborhood"].unique())),
);


Final Wrangle function after module


def wrangle(filepath):
    # Read CSV file
    df = pd.read_csv(filepath)

    # Subset data: Apartments in "Capital Federal", less than 400,000
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    mask_apt = df["property_type"] == "apartment"
    mask_price = df["price_aprox_usd"] < 400_000
    df = df[mask_ba & mask_apt & mask_price]

    # Subset data: Remove outliers for "surface_covered_in_m2"
    low, high = df["surface_covered_in_m2"].quantile([0.1, 0.9])
    mask_area = df["surface_covered_in_m2"].between(low, high)
    df = df[mask_area]

    # Split "lat-lon" column
    df[["lat", "lon"]] = df["lat-lon"].str.split(",", expand=True).astype(float)
    df.drop(columns="lat-lon", inplace=True)

    # Get place name
    df["neighborhood"] = df["place_with_parent_names"].str.split("|", expand=True)[3]
    df.drop(columns="place_with_parent_names", inplace=True)
    
    # Drop features with high null counts
    df.drop(columns = ["floor", "expenses"], inplace = True)
    
    # Drop low and high cardinality variables i.e. non numerical fields containing low or too high numbers of unique values 
    # Dropping as these don't tell us anything distinguishing to use for our model prediction

    df.drop(columns=["operation", "property_type", "currency", "properati_url"], inplace = True)
    

    # Drop columns with leakage
    # These are columns that give us hints / information about our target feature (feature we are trying to predict - price)
    # Don't need these columns as these may not be available in real world data
    df.drop(columns = ["price", "price_aprox_local_currency","price_per_m2","price_usd_per_m2"], inplace = True)
    
    
    
    
    # We see high multicollinearity between 'surface_covered_in_m2', 'surface_total_in_m2' and rooms so 
    # we remove these from the dataframe - leave only one field for analysis
    # Multicolinearity is when there is a strong association / correlation between two or more independent variables in your dataset
    df.drop(columns = ['surface_total_in_m2', 'rooms'], inplace = True)

    return df